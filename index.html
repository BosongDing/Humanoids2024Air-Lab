<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Head movement ASD</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Imitation of human motion achieves natural head movements for
              humanoid robots in an active-speaker detection task</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://research.tilburguniversity.edu/en/persons/bosong-ding" target="_blank">Bosong Ding</a><sup></sup>,</span>
                <span class="author-block">
                  <a href="https://www.crossvalidate.me/" target="_blank">Murat Kirtay</a><sup></sup>,</span>
                  <span class="author-block">
                    <a href="https://spigler.net/giacomo/" target="_blank">Giacomo Spigler</a>
                  </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block">Tilburg University<br>Humanoids 2024</span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/pdf/2407.11915" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                    
                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/BosongDing/Humanoids2024HRI" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link
                <span class="link-block">
                  <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a> -->
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video poster="" id="tree" autoplay controls muted loop height="100%">
        <!-- Your video here -->
        <source src="static/videos/banner.mp4"
        type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        
      </h2>
    </div>
  </div>
</section>
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Head movements are crucial for social human-
human interaction. They can transmit important cues (e.g.,
joint attention, speaker detection) that cannot be achieved
with verbal interaction alone. This advantage also holds for
human-robot interaction. Even though modeling human mo-
tions through generative AI models has become an active
research area within robotics in recent years, the use of
these methods for producing head movements in human-
robot interaction remains underexplored. In this work, we
employed a generative AI pipeline to produce human-like head
movements for a Nao humanoid robot. In addition, we tested
the system on a real-time active-speaker tracking task in a
group conversation setting. Overall, the results show that the
Nao robot successfully imitates human head movements in a
natural manner while actively tracking the speakers during the
conversation
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->


<!-- Image carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
       <div class="item">
        <!-- Your image here -->
        <img src="static/images/1.png" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          Schematics of the proposed method for generating head-gaze movements from human motion data (methods III-A), along with the example
application to an active-speaker gazing task. The models are obtained by first training a variational autoencoder to model human motion data (methods
III-B). Next, a multilayer perceptron (MLP) is trained to map end fixation points of the training trajectories into the corresponding latent vectors z learned by
the encoder (methods III-C). The system can then be used by first converting desired fixation points into latent vectors, which are subsequently transformed
into motion trajectories by the VAE decoder. In the active-speaker fixation task (methods III-D), audiovisual input from a camera mounted on the head of
a Nao robot is inputted to the Light-ASD [8] active-speaker recognition module to retrieve bounding boxes of faces together with a confidence score for
each possible speaker. Fixations are then decided by sampling a softmax distribution of the confidence scores.
        </h2>
      </div>
      <div class="item">
        <!-- Your image here -->
        <img src="static/images/2.png" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          Analysis of the capabilities of the proposed method to synthesize realistic head-gaze movements. (a) We generate and plot 200 random trajectories
by sampling latent vectors from the VAE prior distribution z ∼ N (0, I), and compare them to the human motion data from Figure 3. (b) We evaluate
the capacity of the system to generate desired head-motion trajectories to look at target points. To do so, we select a set of 21 × 21 target fixations on a
grid [−1, 1] × [−1, 1] (normalized yaw/pitch coordinates), and generate trajectories to reach each of them. The final fixation point for each trajectory is
shown as a distortion grid. The MSE in original coordinates is approximately 3.7 degrees. (c) A subset of 5 × 5 fixation points on the same grid (colored
in red in the middle panel) is selected to show the individual trajectories generated to reach each target. Trajectories are divided into segments (alternating
in red and blue color) to show the angular velocities in the yaw and pitch directions at each timestep.
        </h2>
      </div>
      <div class="item">
        <!-- Your image here -->
        <img src="static/images/3.png" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          Example of head movements on a Nao robot using our proposed method versus the default controller. (a-b) Initial and final pictures during a Nao
fixation movement. (c) We track the red marker on Nao’s nose (in pixel coordinates) during the fixation movement, using our method versus the default
controller. (d-e) We report encoder readings from the Nao’s head showing the robot’s yaw and pitch angles while moving to a target configuration (d) or
executing a trajectory (‘target’) generated using our proposed method.
       </h2>
     </div>
   
  </div>
</div>
</div>
</section>
<!-- End image carousel -->



<!-- Video carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Active Speaker Detection with Generative/ default movment.</h2>
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-video1">
          <video poster="" id="video1" autoplay controls muted loop height="100%">
            <!-- Your video file here -->
            <source src="static/videos/gen.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video2">
          <video poster="" id="video2" autoplay controls muted loop height="100%">
            <!-- Your video file here -->
            <source src="static/videos/default.mp4"
            type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End video carousel -->

<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>BibTex Code Upcoming</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
